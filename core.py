"""é‡è¤‡æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯å…¨ä½“ï¼ˆãƒ­ã‚°ãƒ»é€²æ—ãƒ»æ¯”è¼ƒãƒ»å‰å‡¦ç†ï¼‰ã‚’ã¾ã¨ã‚ãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚"""

import hashlib
import json
import os
import random
import shutil
import sys
import threading
import time
import traceback
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime, timedelta

import msvcrt
import numpy as np
import pillow_heif
import psutil
from PIL import Image
from scipy.fftpack import dct as dct_1d
from skimage.metrics import structural_similarity as ssim

from config import (
    DEBUG_LOG_SSIM,
    DEFAULT_SSIM_THRESHOLD,
    LOG_DIR,
    PHASH_THRESHOLD,
    PROGRESS_BAR_WIDTH,
    RESUME_FILE_NAME,
)

pillow_heif.register_heif_opener()

_log_lock = threading.Lock()
_last_log_time = None
_last_log_line = None

CURRENT_PROGRESS = 0
TOTAL_PROGRESS = 1
CURRENT_ETA_STR = "è¨ˆæ¸¬ä¸­"
LOAD_START_TIME = 0.0
LOAD_LAST_PERCENT = -1
PROGRESS_MODE = "none"
LOAD_DONE = 0
LOAD_TOTAL = 1
BASE_START_DONE = 0
BASE_START_TIME = 0.0
MOVE_START_TIME = 0.0
TOTAL_SOURCE_IMAGES = 0
PROCESSED_BASE_COUNT = 0

W_IMAGES = None
W_SIZES = None
W_PATHS = None
W_PHASHES = None
W_RESOLUTIONS = None
W_HASHES = None


def _format_timestamp_with_delta():
    """ãƒ­ã‚°ãƒ©ãƒ™ãƒ«ç”¨ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼ˆç›´è¿‘å‡ºåŠ›ã¨ã®å·®åˆ†ä»˜ãï¼‰ã‚’ç”Ÿæˆã™ã‚‹ã€‚"""
    global _last_log_time
    now = datetime.now()
    ts = now.strftime("%Y%m%d %H%M%S")
    if _last_log_time:
        diff = (now - _last_log_time).total_seconds()
        if diff < 1.0:
            delta_str = "------"
        else:
            h = int(diff // 3600)
            m = int((diff % 3600) // 60)
            s = int(diff % 60)
            delta_str = f"{h:02d}{m:02d}{s:02d}"
    else:
        delta_str = "------"
    _last_log_time = now
    return f"[{ts} {delta_str}]"


def _base_log(msg: str):
    """é€²æ—ãƒãƒ¼ã‚’æ°—ã«ã›ãšã«ç´ ã®ãƒ­ã‚°ã‚’å‡ºåŠ›ã—ã€ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚‚æ›¸ãè¾¼ã‚€ã€‚"""
    global _last_log_line
    log_name = f"imagededuper_{datetime.now().strftime('%Y%m')}.log"
    logfile = os.path.join(LOG_DIR, log_name)

    lines = []
    for raw in msg.splitlines():
        ts = _format_timestamp_with_delta()
        lines.append(f"{ts} {raw}")
    full_msg = "\n".join(lines)

    with _log_lock:
        print(full_msg, flush=True)
        try:
            with open(logfile, "a", encoding="utf-8") as f:
                f.write(full_msg + "\n")
        except Exception as exc:
            print(f"[log-error] ãƒ­ã‚°æ›¸ãè¾¼ã¿å¤±æ•—: {exc}", flush=True)
        if lines:
            _last_log_line = lines[-1]


def _write_traceback_log(desc: str, exc: Exception):
    """ä¾‹å¤–ç™ºç”Ÿæ™‚ã®è©³ç´°ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚’å°‚ç”¨ãƒ•ã‚¡ã‚¤ãƒ«ã¸è¿½è¨˜ã™ã‚‹ã€‚"""
    today = datetime.now().strftime("%Y%m%d")
    tb_file = os.path.join(LOG_DIR, f"error_traceback_{today}.log")
    ts = _format_timestamp_with_delta()
    tb_text = "".join(
        traceback for traceback in __import__("traceback").format_exception(type(exc), exc, exc.__traceback__)
    )

    with _log_lock:
        with open(tb_file, "a", encoding="utf-8") as f:
            f.write("\n" + "â”€" * 50 + "\n")
            f.write(f"{ts} imagededuper - {desc}\n")
            if _last_log_line:
                f.write(_last_log_line + "\n")
            for line in tb_text.strip().splitlines():
                f.write(f"{ts} {line}\n")
            f.write("â”€" * 50 + "\n")


def log(msg: str):
    """é€²æ—ãƒãƒ¼ã‚’ä¸€æ™‚çš„ã«éš ã—ã¦ã‹ã‚‰ãƒ­ã‚°å‡ºåŠ›ã—ã€çµ‚ã‚ã£ãŸã‚‰é€²æ—ã‚’å†æç”»ã™ã‚‹ã€‚"""
    _clear_progress_line()
    _base_log(msg)
    redraw_progress()


def log_processing_stats(label: str):
    """çµŒéæ™‚é–“ã¨å‡¦ç†æ¸ˆã¿åŸºæº–ç”»åƒæ•°ã‚’ã¾ã¨ã‚ã¦ãƒ­ã‚°ã«å‡ºã™ã€‚"""
    elapsed = time.time() - MOVE_START_TIME if MOVE_START_TIME else 0.0
    h = int(elapsed // 3600)
    m = int((elapsed % 3600) // 60)
    s = int(elapsed % 60)
    log(
        f"[{label}] çµŒéæ™‚é–“: {h:02d}:{m:02d}:{s:02d} / "
        f"å‡¦ç†æ¸ˆã¿å…ƒç”»åƒæ•°: {PROCESSED_BASE_COUNT}/{TOTAL_SOURCE_IMAGES}"
    )


def save_resume(resume_path: str, i: int, j: int, moved: set[str], progress: int):
    """å†é–‹ã«å¿…è¦ãªæƒ…å ±ã‚’ JSON å½¢å¼ã§ä¿å­˜ã™ã‚‹ã€‚"""
    data = {
        "i": i,
        "j": j,
        "moved": list(moved),
        "current_progress": progress,
    }
    with open(resume_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def load_resume(resume_path: str):
    """ä¿å­˜æ¸ˆã¿ã®å†é–‹ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿å–ã£ã¦è¿”ã™ã€‚"""
    if not os.path.exists(resume_path):
        return None
    with open(resume_path, "r", encoding="utf-8") as f:
        return json.load(f)


def _clear_progress_line():
    """é€²æ—ãƒãƒ¼ç”¨ã®è¡Œã‚’æ¶ˆå»ã—ã€ã‚«ãƒ¼ã‚½ãƒ«ã‚’å…ˆé ­ã¸æˆ»ã™ã€‚"""
    sys.stdout.write("\r")
    sys.stdout.write("\033[K")
    sys.stdout.flush()


def print_loading_progress(done: int, total: int):
    """èª­ã¿è¾¼ã¿ãƒ•ã‚§ãƒ¼ã‚ºã®é€²æ—ãƒãƒ¼ã‚’æç”»ã™ã‚‹ã€‚"""
    global PROGRESS_MODE, LOAD_DONE, LOAD_TOTAL, CURRENT_ETA_STR, LOAD_LAST_PERCENT
    PROGRESS_MODE = "load"
    LOAD_DONE = done
    LOAD_TOTAL = total

    if total <= 0:
        return

    ratio = max(0.0, min(1.0, done / total))
    percent = int(ratio * 100)

    if done % 10 == 0:
        CURRENT_ETA_STR = compute_load_eta(done, total)

    filled = int(PROGRESS_BAR_WIDTH * ratio)
    bar = "#" * filled + "." * (PROGRESS_BAR_WIDTH - filled)

    sys.stdout.write(
        f"\r[èª­è¾¼] [{bar}] {percent:3d}% ({done}/{total}) çµ‚äº†äºˆå®š {CURRENT_ETA_STR}  [q:â›” ä¸­æ–­]"
    )
    sys.stdout.flush()
    LOAD_LAST_PERCENT = percent


def compute_load_eta(done: int, total: int) -> str:
    """ç¾åœ¨ã®èª­ã¿è¾¼ã¿æšæ•°ã‹ã‚‰çµ‚äº†äºˆå®šæ™‚åˆ»ã‚’æ¨å®šã™ã‚‹ã€‚"""
    if done == 0:
        return "è¨ˆæ¸¬ä¸­"

    elapsed = time.time() - LOAD_START_TIME
    speed = elapsed / done
    remain = total - done
    eta = datetime.now() + timedelta(seconds=remain * speed)
    return eta.strftime("%Y-%m-%d %H:%M:%S")


def print_compare_progress(done: int, total: int):
    """æ¯”è¼ƒãƒ•ã‚§ãƒ¼ã‚ºã®é€²æ—ãƒãƒ¼ã‚’æç”»ã™ã‚‹ã€‚"""
    global CURRENT_PROGRESS, TOTAL_PROGRESS, PROGRESS_MODE
    CURRENT_PROGRESS = done
    TOTAL_PROGRESS = total
    PROGRESS_MODE = "compare"

    if total <= 0:
        return

    ratio = max(0.0, min(1.0, done / total))
    filled = int(PROGRESS_BAR_WIDTH * ratio)
    bar = "#" * filled + "." * (PROGRESS_BAR_WIDTH - filled)
    sys.stdout.write(
        f"\r[é€²æ—] [{bar}] {ratio*100:6.2f}% ({done}/{total}) çµ‚äº†äºˆå®š: {CURRENT_ETA_STR}  [q:â›” ä¸­æ–­]"
    )
    sys.stdout.flush()


def redraw_progress():
    """ç›´å‰ã®ãƒ­ã‚°å‡ºåŠ›ã§æ¶ˆãˆãŸé€²æ—è¡¨ç¤ºã‚’å¾©å…ƒã™ã‚‹ã€‚"""
    if PROGRESS_MODE == "load":
        print_loading_progress(LOAD_DONE, LOAD_TOTAL)
    elif PROGRESS_MODE == "compare":
        print_compare_progress(CURRENT_PROGRESS, TOTAL_PROGRESS)


def update_move_start_time():
    """æ¯”è¼ƒå‡¦ç†ã®é–‹å§‹æ™‚åˆ»ã‚’è¨˜éŒ²ã—ç›´ã™ã€‚"""
    global MOVE_START_TIME
    MOVE_START_TIME = time.time()


def set_total_source_images(count: int):
    """ç·ç”»åƒæšæ•°ã‚«ã‚¦ãƒ³ã‚¿ã‚’æ›´æ–°ã™ã‚‹ã€‚"""
    global TOTAL_SOURCE_IMAGES
    TOTAL_SOURCE_IMAGES = count


def set_processed_base_count(count: int):
    """ç¾åœ¨ã¾ã§ã«æ‰±ã£ãŸåŸºæº–ç”»åƒæ•°ã‚’è¨˜éŒ²ã™ã‚‹ã€‚"""
    global PROCESSED_BASE_COUNT
    PROCESSED_BASE_COUNT = count


def increment_progress(delta: int = 1):
    """æ¯”è¼ƒæ¸ˆã¿ãƒšã‚¢æ•°ã‚’åŠ ç®—ã™ã‚‹ã€‚"""
    global CURRENT_PROGRESS
    CURRENT_PROGRESS += delta


def set_progress(value: int):
    """æ¯”è¼ƒæ¸ˆã¿ãƒšã‚¢æ•°ã‚’ç›´æ¥è¨­å®šã™ã‚‹ã€‚"""
    global CURRENT_PROGRESS
    CURRENT_PROGRESS = value


def _backoff(attempt: int):
    """æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã§ãƒªãƒˆãƒ©ã‚¤å‰ã®å¾…æ©Ÿæ™‚é–“ã‚’èª¿æ•´ã™ã‚‹ã€‚"""
    delay = min(30, (2**attempt) + random.uniform(0, 1))
    time.sleep(delay)


def install_silent_keyboardinterrupt_hook():
    """KeyboardInterrupt ã®ã¨ãã ã‘ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚’æŠ‘åˆ¶ã™ã‚‹ excepthook ã‚’ä»•è¾¼ã‚€ã€‚"""
    old_hook = sys.excepthook

    def _hook(exc_type, exc, tb):
        if exc_type is KeyboardInterrupt:
            return
        old_hook(exc_type, exc, tb)

    sys.excepthook = _hook


def safe(func, *args, desc="å‡¦ç†", retries=0, **kwargs):
    """ä¾‹å¤–ã«å¼·ã„ãƒªãƒˆãƒ©ã‚¤ä»˜ããƒ©ãƒƒãƒ‘ãƒ¼ã§ä»»æ„ã®é–¢æ•°ã‚’å®Ÿè¡Œã™ã‚‹ã€‚"""
    for attempt in range(retries + 1):
        try:
            return func(*args, **kwargs)

        except KeyboardInterrupt:
            raise

        except Exception as exc:
            is_final = attempt >= retries
            msg = f"âš  {desc} å¤±æ•— (è©¦è¡Œ {attempt+1}/{retries+1})"
            log(msg if not is_final else f"{msg} [traceback]")
            if is_final:
                _write_traceback_log(desc, exc)
                log(f"âŒ {desc} å®Œå…¨å¤±æ•—")
                return None
            _backoff(attempt)


def quit_requested():
    """ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã§ q/Q ãŒæŠ¼ã•ã‚Œã¦ã„ãªã„ã‹ã‚’èª¿ã¹ã‚‹ã€‚"""
    return msvcrt.kbhit() and msvcrt.getch() in (b"q", b"Q")


def compute_file_sha1(path: str, chunk: int = 1024 * 1024) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã® SHA-1 ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—ã™ã‚‹ã€‚"""
    h = hashlib.sha1()
    with open(path, "rb") as f:
        while True:
            c = f.read(chunk)
            if not c:
                break
            h.update(c)
    return h.hexdigest()


def safe_rename_with_hash(src: str, dst: str, desc: str) -> bool:
    """rename ã«å¤±æ•—ã—ãŸå ´åˆã§ã‚‚ SHA-1 ç…§åˆã§å®‰å…¨ã«çµ±ä¸€ã™ã‚‹ã€‚"""
    def _try(a, b):
        os.rename(a, b)
        return True

    result = safe(_try, src, dst, desc=desc, retries=2)
    if result:
        return True

    if not os.path.exists(dst):
        return False

    log(f"[è¡çªæ¤œçŸ¥] renameå¤±æ•— â†’ SHA-1æ¯”è¼ƒã¸ {os.path.basename(src)}")

    try:
        h_src = compute_file_sha1(src)
        h_dst = compute_file_sha1(dst)
    except Exception as exc:
        log(f"[ãƒãƒƒã‚·ãƒ¥æ¯”è¼ƒå¤±æ•—] {src} / {dst}: {exc}")
        return False

    if h_src == h_dst:
        log(f"[åŒä¸€åˆ¤å®š] {src} ã¨ {dst} ã¯å†…å®¹ä¸€è‡´ã¨åˆ¤å®š")

        m_src = os.path.getmtime(src)
        m_dst = os.path.getmtime(dst)
        rem, surv = (src, dst) if m_src < m_dst else (dst, src)

        log(f"[å‰Šé™¤] å†…å®¹ä¸€è‡´ â†’ å¤ã„æ–¹ã‚’å‰Šé™¤: {rem}")
        try:
            os.remove(rem)
        except Exception as exc:
            log(f"[å‰Šé™¤å¤±æ•—] {rem}: {exc}")
            return False

        if surv == src:
            try:
                os.rename(src, dst)
            except Exception as exc:
                log(f"[å†renameå¤±æ•—] {src} â†’ {dst}: {exc}")
                return False

        log(f"[çµ±ä¸€å®Œäº†] renameæˆåŠŸ {dst}")
        return True

    log(f"[çµ±ä¸€ã‚¹ã‚­ãƒƒãƒ—] SHA-1ä¸ä¸€è‡´ {src} / {dst}")
    return False


def convert_heic_to_jpg(path: str, dup_dir: str) -> str | None:
    """HEIC/HEIF ã‚’ JPEG ã«å¤‰æ›ã—ã€å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ duplicates ã¸é€€é¿ã™ã‚‹ã€‚"""
    try:
        with Image.open(path) as img:
            new_path = os.path.splitext(path)[0] + ".jpg"
            img.convert("RGB").save(new_path, "JPEG", quality=95)
        dst = os.path.join(dup_dir, os.path.basename(path))
        shutil.move(path, dst)
        log(f"[ğŸ”„ HEICâ†’JPG] {path} â†’ {new_path}")
        return new_path
    except Exception as exc:
        log(f"[âŒ HEICå¤‰æ›å¤±æ•—] {path}: {exc}")
        return None


def rename_jfif_to_jpg(path: str) -> str:
    """æ‹¡å¼µå­ .jfif ã‚’ .jpg ã«å¤‰æ›´ã™ã‚‹ã€‚"""
    try:
        new_path = os.path.splitext(path)[0] + ".jpg"
        os.rename(path, new_path)
        log(f"[ğŸ”„ JFIFâ†’JPG] {path} â†’ {new_path}")
        return new_path
    except Exception as exc:
        log(f"[âŒ JFIFâ†’JPGå¤±æ•—] {path}: {exc}")
        return path


def fix_wrong_extension(path: str) -> str:
    """å®Ÿéš›ã®ç”»åƒãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«åˆã‚ã›ã¦æ‹¡å¼µå­ã‚’è£œæ­£ã™ã‚‹ã€‚"""
    try:
        with Image.open(path) as img:
            fmt = (img.format or "").upper()
        current_ext = os.path.splitext(path)[1].lower()

        if fmt in ("JPEG", "JFIF"):
            new_path = os.path.splitext(path)[0] + ".jpg"
            if current_ext == ".jpg":
                return path
            ok = safe_rename_with_hash(path, new_path, desc="æ‹¡å¼µå­çµ±ä¸€(JPEG)")
            return new_path if ok else path

        ext_map = {"PNG": ".png", "GIF": ".gif", "WEBP": ".webp", "TIFF": ".tiff", "BMP": ".bmp"}
        if fmt in ext_map:
            correct = ext_map[fmt]
            if current_ext == correct:
                return path
            new_path = os.path.splitext(path)[0] + correct
            ok = safe_rename_with_hash(path, new_path, desc=f"[ğŸ”§ æ‹¡å¼µå­ä¿®æ­£]({fmt})")
            return new_path if ok else path

        return path

    except Exception as exc:
        log(f"[âŒ æ‹¡å¼µå­åˆ¤å®šå¤±æ•—] {path}: {exc}")
        return path


def dct2(a: np.ndarray) -> np.ndarray:
    """2 æ¬¡å…ƒ DCT ã‚’è¨ˆç®—ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼ã€‚"""
    return dct_1d(dct_1d(a, axis=0, norm="ortho"), axis=1, norm="ortho")


def calc_phash(img_arr: np.ndarray) -> int:
    """224x224 ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ç”»åƒã‹ã‚‰ 64bit ã® pHash ã‚’è¨ˆç®—ã™ã‚‹ã€‚"""
    img = Image.fromarray(img_arr).resize((32, 32), Image.LANCZOS)
    mat = np.asarray(img, dtype=np.float32)
    d = dct2(mat)
    d_low = d[:8, :8]
    med = np.median(d_low)
    bits = (d_low > med).flatten()
    value = 0
    for bit in bits:
        value = (value << 1) | int(bool(bit))
    return value


def hamming64(a: int, b: int) -> int:
    """64bit æ•´æ•°åŒå£«ã®ãƒãƒŸãƒ³ã‚°è·é›¢ã‚’æ±‚ã‚ã‚‹ã€‚"""
    return (a ^ b).bit_count()


def cache_all_images(paths: list[str]):
    """å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€SSIM ç”¨ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¸€å¼ã‚’ä½œã£ã¦è¿”ã™ã€‚"""
    global LOAD_START_TIME, LOAD_LAST_PERCENT, CURRENT_ETA_STR
    LOAD_START_TIME = time.time()
    LOAD_LAST_PERCENT = -1
    imgs = []
    sizes = []
    phashes = []
    valid_paths = []
    resolutions = []
    hashes = []

    total = len(paths)
    log(f"[ğŸ“¥ èª­è¾¼é–‹å§‹] {total} æš")
    CURRENT_ETA_STR = "è¨ˆæ¸¬ä¸­"

    for idx, path in enumerate(paths, start=1):
        if quit_requested():
            log("èª­ã¿è¾¼ã¿ä¸­ã«ä¸­æ–­æ“ä½œã‚’æ¤œçŸ¥ã—ãŸã‚ã€‚")
            return None

        print_loading_progress(idx, total)

        if not os.path.exists(path):
            continue

        try:
            with Image.open(path) as img:
                width, height = img.size
                g = img.convert("L")
                arr = np.array(g.resize((224, 224)))
        except Exception as exc:
            log(f"[âš  èª­è¾¼å¤±æ•—] {path}: {exc}")
            continue

        imgs.append(arr)
        sizes.append(os.path.getsize(path))
        resolutions.append((width, height))
        phashes.append(calc_phash(arr))
        valid_paths.append(path)
        try:
            hashes.append(compute_file_sha1(path))
        except Exception as exc:
            log(f"[âš  SHA-1è¨ˆç®—å¤±æ•—] {path}: {exc}")
            hashes.append("")

    _clear_progress_line()
    log(f"[âœ… èª­è¾¼å®Œäº†] æœ‰åŠ¹ç”»åƒæ•°: {len(valid_paths)}")
    return valid_paths, imgs, sizes, phashes, resolutions, hashes


def get_optimal_workers():
    """CPU ã‚³ã‚¢æ•°ã‹ã‚‰ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ã®é©åˆ‡ãª worker æ•°ã‚’æ¨å®šã™ã‚‹ã€‚"""
    phys = psutil.cpu_count(logical=False)
    logi = psutil.cpu_count(logical=True)

    if phys == 2 and logi == 4:
        return 4
    return max(2, min(logi, int(phys * 1.3)))


hamming_cache = {}


def fast_hamming(a, b):
    """pHash ã®ãƒãƒŸãƒ³ã‚°è·é›¢ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãã§é«˜é€Ÿã«æ±‚ã‚ã‚‹ã€‚"""
    key = (a << 64) | b
    if key in hamming_cache:
        return hamming_cache[key]
    distance = hamming64(a, b)
    hamming_cache[key] = distance
    return distance


def set_worker_data(images, sizes, paths, phashes, resolutions, hashes):
    """æ¯”è¼ƒãƒ¯ãƒ¼ã‚«ãƒ¼ãŒå‚ç…§ã™ã‚‹ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’è¨­å®šã™ã‚‹ã€‚"""
    global W_IMAGES, W_SIZES, W_PATHS, W_PHASHES, W_RESOLUTIONS, W_HASHES
    W_IMAGES = images
    W_SIZES = sizes
    W_PATHS = paths
    W_PHASHES = phashes
    W_RESOLUTIONS = resolutions
    W_HASHES = hashes


def clear_worker_data():
    """æ¯”è¼ƒãƒ¯ãƒ¼ã‚«ãƒ¼ç”¨ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹ã€‚"""
    set_worker_data(None, None, None, None, None, None)


def ssim_task(pair):
    """1 ãƒšã‚¢ã® SHA-1 / pHash / SSIM åˆ¤å®šã‚’è¡Œã„ã€çµæœã‚’è¿”ã™ã€‚"""
    try:
        i, j = pair
        sha_match = False

        if W_HASHES[i] and W_HASHES[i] == W_HASHES[j]:
            sha_match = True
        else:
            if fast_hamming(W_PHASHES[i], W_PHASHES[j]) > PHASH_THRESHOLD:
                return None

        if sha_match:
            score = 1.0
        else:
            img1 = W_IMAGES[i]
            img2 = W_IMAGES[j]
            score = float(ssim(img1, img2, full=False))

            if DEBUG_LOG_SSIM:
                log(f"[DEBUG SSIM] {W_PATHS[i]} vs {W_PATHS[j]} -> SSIM={score:.4f}")

        return (
            i,
            j,
            W_PATHS[i],
            W_PATHS[j],
            score,
            W_SIZES[i],
            W_SIZES[j],
            W_RESOLUTIONS[i],
            W_RESOLUTIONS[j],
            sha_match,
        )

    except KeyboardInterrupt:
        return "INTERRUPT"

    except Exception:
        return None


def compute_next_pair(i: int, j: int, n: int) -> tuple[int, int]:
    """ç¾åœ¨ã® (i, j) ã«ç¶šãæ¯”è¼ƒãƒšã‚¢ã‚’è¨ˆç®—ã™ã‚‹ã€‚"""
    j = j + 1
    if j <= i:
        j = i + 1

    if j >= n:
        i += 1
        if i >= n - 1:
            return n - 1, n
        j = i + 1

    return i, j


def move_duplicates(folder_path: str, threshold: float = DEFAULT_SSIM_THRESHOLD):
    """æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€å†…ã®é‡è¤‡ç”»åƒã‚’æ¤œå‡ºã—ã€duplicates ã¸ç§»å‹•ã™ã‚‹ãƒ¡ã‚¤ãƒ³å‡¦ç†ã€‚"""
    resume_path = os.path.join(folder_path, RESUME_FILE_NAME)
    resume = load_resume(resume_path)
    is_resume = resume is not None

    if is_resume:
        start_i = int(resume["i"])
        start_j = int(resume["j"])
        moved = set(resume["moved"])
        current_progress = int(resume.get("current_progress", 0))
        set_progress(current_progress)
        set_processed_base_count(start_i)
        log(f"[â¸ï¸â†’â–¶ï¸ å†é–‹] i={start_i}, j={start_j} ã‹ã‚‰å†é–‹ã™ã‚‹ã‚ã€‚")
    else:
        start_i = 0
        start_j = 1
        moved = set()
        current_progress = 0
        set_progress(0)
        set_processed_base_count(0)

    moved_before = set(moved)

    log(f"=== é–‹å§‹: é‡è¤‡ç”»åƒãƒã‚§ãƒƒã‚¯ {folder_path} ===")
    update_move_start_time()
    set_total_source_images(0)
    set_processed_base_count(0)

    exts = (".jpg", ".jpeg", ".png", ".bmp", ".gif", ".tiff", ".webp", ".jfif", ".heic", ".heif")

    dup_dir = os.path.join(folder_path, "duplicates")
    os.makedirs(dup_dir, exist_ok=True)

    all_files: list[str] = []
    for root, dirs, filenames in os.walk(folder_path):
        dirs[:] = [d for d in dirs if d.lower() != "duplicates"]
        for f in filenames:
            if f.lower().endswith(exts):
                all_files.append(os.path.join(root, f))

    if not all_files:
        log("ç”»åƒãŒ1æšã‚‚ãªã„ã®ã§çµ‚äº†ã™ã‚‹ã‚ã€‚")
        log_processing_stats("å®Œäº†")
        return

    log(f"[åé›†] {len(all_files)} æš")

    estimated_mem_mb = len(all_files) * 0.05
    log(f"[äºˆå®šãƒ¡ãƒ¢ãƒªæ¶ˆè²»] ç´„ {estimated_mem_mb:.2f} MBï¼ˆ224x224 ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰")

    after = []
    for f in all_files:
        if f.lower().endswith((".heic", ".heif")):
            new = safe(convert_heic_to_jpg, f, dup_dir, desc="HEICå¤‰æ›", retries=2)
            if new:
                after.append(new)
            continue
        after.append(f)

    tmp = []
    for f in after:
        if f.lower().endswith(".jfif"):
            new = safe(rename_jfif_to_jpg, f, desc="JFIFâ†’JPG", retries=2)
            tmp.append(new if new else f)
            continue
        tmp.append(f)

    final = []
    for f in tmp:
        fixed = safe(fix_wrong_extension, f, desc="æ‹¡å¼µå­ä¿®æ­£", retries=2)
        final.append(fixed if fixed else f)

    cached = cache_all_images(final)
    if cached is None:
        log("èª­ã¿è¾¼ã¿ãŒä¸­æ–­ã•ã‚ŒãŸã‹ã‚‰ã€æ¯”è¼ƒå‡¦ç†ã«ã¯é€²ã¾ãšã«çµ‚äº†ã™ã‚‹ã‚ã€‚")
        log_processing_stats("ä¸­æ–­")
        return

    cached_paths, cached_images, cached_sizes, cached_phashes, cached_resolutions, cached_hashes = cached
    n = len(cached_paths)
    set_total_source_images(n)
    set_processed_base_count(0)

    order = sorted(range(n), key=lambda x: cached_phashes[x])
    cached_paths = [cached_paths[i] for i in order]
    cached_images = [cached_images[i] for i in order]
    cached_sizes = [cached_sizes[i] for i in order]
    cached_phashes = [cached_phashes[i] for i in order]
    cached_resolutions = [cached_resolutions[i] for i in order]
    cached_hashes = [cached_hashes[i] for i in order]

    if n < 2:
        log("æ¯”è¼ƒå¯¾è±¡ãŒ1æšã—ã‹ãªã„ã‹ã‚‰å‡¦ç†ã™ã‚‹ã“ã¨ãŒãªã„ã‚ã€‚")
        set_processed_base_count(n)
        log_processing_stats("å®Œäº†")
        return

    total_pairs = n * (n - 1) // 2
    workers = get_optimal_workers()
    log(f"[æ¯”è¼ƒè¨­å®š] ç”»åƒæ•°={n}, çµ„ã¿åˆã‚ã›={total_pairs}, workers={workers}")
    log("[ğŸ” æ¯”è¼ƒ] pHash ã§å€™è£œã‚’çµã‚Šã€ãã®ä¸­ã ã‘ SSIM ã§æœ€çµ‚åˆ¤å®šã™ã‚‹ã‚ã€‚")

    if not is_resume:
        current_progress = 0
        set_progress(0)
    global TOTAL_PROGRESS
    TOTAL_PROGRESS = total_pairs
    print_compare_progress(current_progress, total_pairs)

    def on_new_base(i: int):
        global CURRENT_ETA_STR
        set_processed_base_count(i)
        if current_progress <= 0 or current_progress < total_pairs * 0.01:
            CURRENT_ETA_STR = "è¨ˆæ¸¬ä¸­"
        else:
            elapsed = time.time() - MOVE_START_TIME
            avg_speed = current_progress / max(elapsed, 1e-6)
            remaining_pairs = total_pairs - current_progress
            rem_ratio = remaining_pairs / total_pairs
            accel = rem_ratio ** 0.5
            est_sec = (remaining_pairs / max(avg_speed, 1e-6)) * accel
            eta_dt = datetime.now() + timedelta(seconds=max(0.0, est_sec))
            CURRENT_ETA_STR = eta_dt.strftime("%Y-%m-%d %H:%M:%S")

        save_resume(resume_path, i, 0, moved, current_progress)
        log(f"[æ¯”è¼ƒ] åŸºæº–ç”»åƒ {i+1}/{n}: {os.path.basename(cached_paths[i])}")

    def pair_gen():
        current_i = None
        i = start_i if is_resume else 0
        j = start_j if is_resume else 1

        while i < n:
            if i != current_i:
                current_i = i
                on_new_base(i)

            if j >= n:
                i += 1
                if i >= n:
                    break
                j = i + 1
                continue

            yield (i, j)
            j += 1

    pairs = pair_gen()
    max_pending = workers * 2
    last_i = start_i
    last_j = start_i

    set_worker_data(cached_images, cached_sizes, cached_paths, cached_phashes, cached_resolutions, cached_hashes)
    exe = ThreadPoolExecutor(max_workers=workers)

    try:
        pending = set()
        try:
            while len(pending) < max_pending:
                pair = next(pairs)
                fut = exe.submit(ssim_task, pair)
                pending.add((pair, fut))
        except StopIteration:
            pass

        while pending:
            if quit_requested():
                raise KeyboardInterrupt

            done_futs = []
            for pair, fut in list(pending):
                if fut.done():
                    done_futs.append((pair, fut))
                    pending.remove((pair, fut))

            if not done_futs:
                if quit_requested():
                    raise KeyboardInterrupt
                time.sleep(0.01)
                continue

            for pair, fut in done_futs:
                i, j = pair
                last_i, last_j = i, j

                res = fut.result()

                if res == "INTERRUPT":
                    raise KeyboardInterrupt

                if not res:
                    current_progress += 1
                    set_progress(current_progress)
                    print_compare_progress(current_progress, total_pairs)
                    continue

                i, j, a, b, score, sa, sb, ra, rb, sha_match = res
                current_progress += 1
                set_progress(current_progress)
                print_compare_progress(current_progress, total_pairs)

                if current_progress % 10 == 0:
                    ni, nj = compute_next_pair(i, j, n)
                    save_resume(resume_path, ni, nj, moved, current_progress)

                if a in moved or b in moved:
                    continue

                if sha_match or score >= threshold:
                    (width_a, height_a) = ra
                    (width_b, height_b) = rb

                    res_a = width_a * height_a
                    res_b = width_b * height_b

                    smaller = a if res_a < res_b else b

                    dst = os.path.join(dup_dir, os.path.basename(smaller))
                    moved.add(smaller)
                    if sha_match:
                        log(f"[â™» SHA-1ä¸€è‡´] {smaller} ã‚’ç§»å‹•ã™ã‚‹ã‚ã€‚")
                    else:
                        log(f"[ğŸ§© é‡è¤‡æ¤œå‡º] SSIM={score:.4f} â†’ {smaller} ã‚’ç§»å‹•")
                    safe(shutil.move, smaller, dst, desc="é‡è¤‡ç§»å‹•", retries=2)

            try:
                while len(pending) < max_pending:
                    pair = next(pairs)
                    fut = exe.submit(ssim_task, pair)
                    pending.add((pair, fut))
            except StopIteration:
                pass

    except (Exception, KeyboardInterrupt):
        exe.shutdown(wait=False, cancel_futures=True)

        ni, nj = compute_next_pair(last_i, last_j, n)
        save_resume(resume_path, ni, nj, moved, current_progress)
        log_processing_stats("ä¸­æ–­")
        log("ä¸­æ–­æ“ä½œã‚’æ¤œçŸ¥ã—ãŸã‹ã‚‰ã€ä¸­æ–­ä½ç½®ã‚’ä¿å­˜ã—ã¦çµ‚äº†ã™ã‚‹ã‚ã€‚")
        return

    finally:
        exe.shutdown(wait=False, cancel_futures=True)
        clear_worker_data()

    if os.path.exists(resume_path):
        os.remove(resume_path)
        log("[ğŸ—‘ å†é–‹ãƒ‡ãƒ¼ã‚¿å‰Šé™¤] æ­£å¸¸çµ‚äº†ã—ãŸã‹ã‚‰ resume.json ã‚’å‰Šé™¤ã—ãŸã‚ã€‚")

    set_processed_base_count(n)
    log_processing_stats("å®Œäº†")
    log("=== ğŸ‰ å®Œäº† ===")

    new_moved_count = len(moved) - len(moved_before)
    log(f"ä»Šå›ç§»å‹•ã—ãŸé‡è¤‡ç”»åƒæšæ•°: {new_moved_count}")
